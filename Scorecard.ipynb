{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff6aae60",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1f99dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0f12ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scorecardpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pandasql import sqldf\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scorecardpy.LogisticRegStats import LogisticRegStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946ebf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# import the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, precision_recall_curve, auc\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy.stats import chi2_contingency\n",
    "import scorecardpy as sc\n",
    "from scorecardpy.LogisticRegStats import LogisticRegStats\n",
    "import random as rd\n",
    "import re\n",
    "from IPython.display import display\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fa21b6",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04f3004",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data prepare ------\n",
    "# load germancredit data\n",
    "smp_full = sc.germancredit()\n",
    "smp_full['target'] = smp_full['creditability'].apply(lambda x: 1 if x == 'bad' else 0)\n",
    "smp_full = smp_full.drop(columns = ['creditability'])\n",
    "smp_full.loc[0:99, 'credit_amount'] = np.nan\n",
    "smp_full.loc[0:99, 'purpose'] = np.nan\n",
    "smp_full.loc[100:109, 'target'] = np.nan\n",
    "smp_full.loc[80:100, 'age_in_years'] = -9999999\n",
    "smp_full.loc[40:80, 'age_in_years'] = np.nan\n",
    "\n",
    "for i in range(5):\n",
    "    smp_full = pd.concat([smp_full, smp_full])\n",
    "smp_full['RepDate_End'] = np.random.randint(1, 73, smp_full.shape[0])\n",
    "smp_full = smp_full.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d718b37",
   "metadata": {},
   "source": [
    "# 1. Exploratory analysis of variables (missings, outliers, concentration/distribution) - based on smp_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630c601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# good/bad label\n",
    "target = 'target'\n",
    "\n",
    "# date column (e.g. snapshot date or application date)\n",
    "date = 'RepDate_End'\n",
    "\n",
    "# other columns that are not variables\n",
    "var_skip = []\n",
    "\n",
    "# all columns that are not variables\n",
    "var_skip_all = var_skip + [target, date]\n",
    "\n",
    "# special values for numeric variables - TBD\n",
    "special_values = [-9999999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676b89ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap for the missing values\n",
    "sc.miss_heatmap(smp_full, var_skip, fig_width=10, fig_height=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e025b9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# variables checks summary\n",
    "var_cat_summary, var_num_summary, var_list = sc.expl_analysis(smp_full, var_skip_all, special_values)\n",
    "\n",
    "display(var_cat_summary)\n",
    "display(var_num_summary)\n",
    "\n",
    "# var_max_share = var_cat_summary[var_cat_summary['Max share'] > 0.95]['Variable'].tolist() \\\n",
    "#              + var_num_summary[var_num_summary['Max share'] > 0.95]['Variable'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dba7c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# treatment of nan - median for numeric and 'Missing' for string\n",
    "#smp_full2 = sc.nan_treatment(smp_full, x = None, var_skip = var_skip_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521906be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# variables distribution\n",
    "sc.var_distr(smp_full, var_list, groupby = target, special_values = special_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a75c1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis of shares of missings and bads in target over time\n",
    "def nan_rate(target): return sum(np.isnan(target)) / len(target)\n",
    "def bad_rate(target): return sum(target==1) / (sum(target==0)+sum(target==1))\n",
    "target_ot = smp_full.groupby(date)[target].agg([nan_rate, bad_rate])\n",
    "\n",
    "# dates with blank target\n",
    "pd.DataFrame(target_ot[target_ot['nan_rate'] > 0]['nan_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6274565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad rate over time\n",
    "target_ot['bad_rate'].plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c723d2",
   "metadata": {},
   "source": [
    "# 2. Development sample creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0db2cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection of the development window \n",
    "smp_dev = smp_full[smp_full[date].between(1, 60)]\n",
    "\n",
    "# selection of variables that will be used for the development\n",
    "smp_dev = smp_dev[var_list + [target, date]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee8303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check target\n",
    "print(smp_dev.groupby(target, dropna=False).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65ac1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete records with blank target\n",
    "smp_dev = smp_dev[smp_dev[target].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7011aca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split as 80/20\n",
    "train, test = sc.split_df(smp_dev, ratio=0.8, seed=123).values()\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce8d0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test sample size\n",
    "query = \"\"\" select 'train' as sample, \n",
    "                sum(target) as bads, \n",
    "                count(*) as obs, \n",
    "                sum(target)*1.00/count(target) as BR\n",
    "            from train \n",
    "                union \n",
    "            select 'test' as sample, \n",
    "                sum(target) as bads, \n",
    "                count(*) as obs, \n",
    "                sum(target)*1.00/count(target) as BR\n",
    "            from test \n",
    "        \"\"\"\n",
    "# Query execution\n",
    "sqldf(query)\n",
    "# pd.DataFrame({'train':pd.Series(train.groupby('target', dropna=False).size()),\n",
    "#               'test':pd.Series(test.groupby('target', dropna=False).size())})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7e95c3",
   "metadata": {},
   "source": [
    "# 3. Automated binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e152f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binning\n",
    "fine_class, coarse_class = sc.woebin(train, y = target,\n",
    "                                  #x = [\"age_in_years\", \"status_of_existing_checking_account\", \"foreign_worker\"],\n",
    "                                  var_skip = var_skip_all,\n",
    "                                  special_values = special_values,\n",
    "                                  min_perc_fine_bin = 0.05, # min bin size for fine classing\n",
    "                                  count_distr_limit = 0.1, # min bin size for coarse classing\n",
    "                                  bin_num_limit = 10, # max number of coarse classes\n",
    "                                  print_step = 5,\n",
    "                                  ignore_datetime_cols = False,\n",
    "                                  bin_decimals = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec77cc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting binning results to excel\n",
    "pd.concat(fine_class.values()).reset_index(drop=True).to_excel('3_1_fine_classing.xlsx')\n",
    "pd.concat(coarse_class.values()).reset_index(drop=True).to_excel('3_2_coarse_classing_auto.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eb6448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iv for variables after automated binning\n",
    "coarse_class_iv = sc.vars_iv(coarse_class)\n",
    "coarse_class_iv.to_excel('3_3_coarse_classing_auto_iv.xlsx')\n",
    "coarse_class_iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934ff23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_list, var_rej_fine = sc.vars_filter(train, fine_class, corr_threshold = 0.5, iv_threshold = 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b41cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binning visualization\n",
    "# sc.woebin_plot(coarse_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af20d35d",
   "metadata": {},
   "source": [
    "# 4. Binning adjustments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e763453f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# manual review and adjustment of binning (first line should uncommented if manual adjustments are needed to be done) !update\n",
    "breaks_list = sc.woebin_adj(train, y=target, \n",
    "                            bins=coarse_class, \n",
    "                            init_bins=fine_class, \n",
    "                            adj_all_var=False, \n",
    "                            show_init_bins=False,\n",
    "                            save_breaks_list='3_4_breaks_list_adj.py')\n",
    "# %run 3_4_breaks_list_adj.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29841728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables excluded based on binning results\n",
    "vars_bin_excl = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3984c9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update of coarse classing table (fine classing is relevant only for automated binning)\n",
    "fine_class_adj, coarse_class_adj = sc.woebin(train, y = 'target', \n",
    "                                  #x = [\"age_in_years\", \"status_of_existing_checking_account\", \"foreign_worker\"],\n",
    "                                  breaks_list = breaks_list, \n",
    "                                  var_skip = var_skip_all,\n",
    "                                  special_values = special_values,\n",
    "                                  min_perc_fine_bin = 0.05, # min bin size for fine classing\n",
    "                                  count_distr_limit = 0.1, # min bin size for coarse classing\n",
    "                                  bin_num_limit = 10, # max number of coarse classes\n",
    "                                  print_step = 5,\n",
    "                                  ignore_datetime_cols = False,\n",
    "                                  bin_decimals = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9f9fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying woe transformations on train and test samples \n",
    "train_woe = sc.woebin_ply(train, bins=coarse_class_adj)\n",
    "test_woe = sc.woebin_ply(test, bins=coarse_class_adj)\n",
    "\n",
    "# defining woe variables\n",
    "vars_woe = []\n",
    "for i in var_list:\n",
    "    if i not in vars_bin_excl:\n",
    "        vars_woe.append(i+'_woe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3599e2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results of the final coarse classing after manual adjustments !update\n",
    "pd.concat(coarse_class_adj.values()).reset_index(drop=True).to_excel('3_5_coarse_classing_final.xlsx')\n",
    "coarse_class_adj_iv = sc.vars_iv(coarse_class_adj)\n",
    "coarse_class_adj_iv.to_excel('3_6_coarse_classing_final_iv.xlsx')\n",
    "coarse_class_adj_iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bded02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IV for variables by defined subsamples (period, product etc.)\n",
    "# sc.iv_group(train_woe, \n",
    "#             var_list = [\"age_in_years_woe\"], \n",
    "#             groupby = \"personal_status_and_sex\", \n",
    "#             y = target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d796f435",
   "metadata": {},
   "source": [
    "# 5. Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48592c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "train_woe_corr = train_woe[vars_woe].corr()\n",
    "train_woe_corr.to_excel('5_1_correlation_matrix.xlsx')\n",
    "train_woe_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ba67d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting correlation heatmap\n",
    "plt.figure(figsize=(20,12))\n",
    "sns.heatmap(train_woe[vars_woe].corr(), cmap=\"YlGnBu\", annot=True)\n",
    "  \n",
    "# displaying heatmap\n",
    "plt.savefig('5_2_correlation_heatmap.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1febac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclusions by corr > 0.7\n",
    "vars_corr_excl = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce47aa0",
   "metadata": {},
   "source": [
    "# 6. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a4ae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# values in the test sample that are not present in train\n",
    "#print(np.am=ny(no.isnan(test_woe)))\n",
    "test_woe = test_woe.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7052a853",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_woe = []\n",
    "for i in var_list:\n",
    "    if i not in vars_bin_excl + vars_corr_excl:\n",
    "        vars_woe.append(i+'_woe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9271812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target and variables\n",
    "y_train = train_woe['target']\n",
    "X_train = train_woe[vars_woe]\n",
    "y_test = test_woe['target']\n",
    "X_test = test_woe[vars_woe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07d2870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression ------\n",
    "lr = LogisticRegression(penalty='l1', C=0.9, solver='saga', n_jobs=-1)\n",
    "lr.fit(X_train, y_train)\n",
    "# lr.coef_\n",
    "# lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6949e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted proability\n",
    "train_pred = lr.predict_proba(X_train)[:,1]\n",
    "test_pred = lr.predict_proba(X_test)[:,1]\n",
    "# performance ks & roc ------\n",
    "train_perf = sc.perf_eva(y_train, train_pred, title = \"train\")\n",
    "test_perf = sc.perf_eva(y_test, test_pred, title = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7047c09a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train bad rate\n",
    "train_br = {}\n",
    "train_br['Total'] = y_train.count()\n",
    "train_br['Bads'] = int(y_train.sum())\n",
    "train_br['Bad Rate'] = round(train_br['Bads']/train_br['Total'], 4)\n",
    "# test bad rate\n",
    "test_br = {}\n",
    "test_br['Total'] = y_test.count()\n",
    "test_br['Bads'] = int(y_test.sum())\n",
    "test_br['Bad Rate'] = round(test_br['Bads']/test_br['Total'], 4)\n",
    "test_br\n",
    "# combining bad rate with performance\n",
    "perf = pd.DataFrame({'train':pd.Series(dict(list(train_br.items()) + list(train_perf.items()))),\n",
    "                         'test':pd.Series(dict(list(test_br.items()) + list(test_perf.items())))})\n",
    "perf = perf.loc[~perf.index.isin(['pic'])]\n",
    "perf.to_excel('6_1_perf_train_test.xlsx')\n",
    "perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976b635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression with stats\n",
    "lr2 = LogisticRegStats(penalty='l1', C=0.9, solver='saga', n_jobs=-1)\n",
    "lr2.fit(X_train, y_train)\n",
    "\n",
    "# calculating p-values and exporting to excel\n",
    "lr_output = pd.DataFrame({\n",
    "                'Variable': ['intercept'] + X_train.columns.tolist(),\n",
    "                'Coefficient': [lr2.model.intercept_[0]] + lr2.model.coef_[0].tolist(),\n",
    "                'P-value': [0] + lr2.p_values\n",
    "                })\n",
    "\n",
    "lr_output.to_excel('6_2_regr_output.xlsx')\n",
    "lr_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1c3d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score ------\n",
    "card = sc.scorecard(coarse_class_adj, lr, X_train.columns, basepoints_eq0=False)\n",
    "# credit score\n",
    "train_score  = sc.scorecard_ply(train, card, print_step=0)\n",
    "test_score = sc.scorecard_ply(test, card, print_step=0)\n",
    "# output to excel\n",
    "scorecard_points = pd.concat(card, ignore_index=True)\n",
    "scorecard_points.to_excel(\"6_3_scorecard_points.xlsx\", sheet_name='scorecard_points')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257f8e89",
   "metadata": {},
   "source": [
    "# 7. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd898c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "smp_dev_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e9ad5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# samples scoring\n",
    "smp_dev_score = sc.woebin_ply(smp_dev, bins=coarse_class_adj)\n",
    "smp_dev_score['score'] = sc.scorecard_ply(smp_dev, card, print_step=0)\n",
    "\n",
    "smp_full_score = sc.woebin_ply(smp_full, bins=coarse_class_adj)\n",
    "smp_full_score['score'] = sc.scorecard_ply(smp_full, card, print_step=0)\n",
    "\n",
    "# adding target\n",
    "train_score[target] = train[target]\n",
    "test_score[target] = test[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406a53cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bad Rate over time\n",
    "gini_ot = smp_dev_score[[date, target]].groupby([date]).agg(['count', 'sum' ])\n",
    "gini_ot = gini_ot.rename(columns={\"count\": \"Total\", \"sum\": \"Bads\"})\n",
    "gini_ot.columns = gini_ot.columns.droplevel(0)\n",
    "gini_ot['Bad Rate'] = (gini_ot['Bads'] / gini_ot['Total'])\n",
    "# adding Gini over time\n",
    "gini_ot['Gini'] = sc.gini_over_time(smp_dev_score, target, ['score'], date)\n",
    "\n",
    "# Gini for vars train/test\n",
    "gini_vars_train = sc.gini_vars(train_woe, target, vars_woe, 'Train')\n",
    "gini_vars_test = sc.gini_vars(test_woe, target, vars_woe, 'Test')\n",
    "gini_vars_train_test = pd.merge(gini_vars_train, gini_vars_test, on = \"Variable\")\n",
    "\n",
    "# Gini for vars over time\n",
    "gini_vars_ot = sc.gini_over_time(smp_dev_score, target, vars_woe, date)\n",
    "                                 \n",
    "# defining score ranges on train sample\n",
    "_, brk = pd.cut(train_score['score'], bins=10, retbins=True, duplicates='drop')\n",
    "brk = brk.round(decimals=2)\n",
    "brk = list(filter(lambda x: x>np.nanmin(train_score['score']) and x<np.nanmax(train_score['score']), brk))\n",
    "brk = [np.nanmin(smp_full_score['score'])] + sorted(brk) + [np.nanmax(smp_full_score['score'])]\n",
    "# applying score ranges on train, test adn full samples\n",
    "train_score['score_range'] = pd.cut(train_score['score'], bins=brk, include_lowest=False)\n",
    "test_score['score_range'] = pd.cut(test_score['score'], bins=brk, include_lowest=False)\n",
    "smp_full_score['score_range'] = pd.cut(smp_full_score['score'], bins=brk, include_lowest=False)\n",
    "# score distribution for train/test\n",
    "train_distr = sc.score_distr(train_score, target, 'score', 'score_range')\n",
    "test_distr = sc.score_distr(test_score, target, 'score', 'score_range')\n",
    "                                 \n",
    "# PSI over time (score_range) \n",
    "psi_ot = sc.psi_over_time(train_score, smp_full_score, ['score_range'], date)\n",
    "                                 \n",
    "# PSI for WoE variables over time\n",
    "psi_vars_ot = sc.psi_over_time(train_woe, smp_full_score, vars_woe, date)\n",
    "                                 \n",
    "# calculating hhi for train/test\n",
    "train_hhi = sc.hhi(train_score['score_range'].astype(str))\n",
    "test_hhi = sc.hhi(test_score['score_range'].astype(str))\n",
    "hhi_train_test = pd.DataFrame({\n",
    "        'train': [train_hhi], \n",
    "        'test': [test_hhi]\n",
    "    }, index = ['hhi'])\n",
    "                                 \n",
    "# calculating hhi over time\n",
    "hhi_ot = smp_full_score.groupby(date).agg({'score_range': sc.hhi})\n",
    "hhi_ot = hhi_ot.rename(columns={\"score_range\": \"HHI\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ad6eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning ratings\n",
    "bins = [0,500,540,580,620,660,700,740,780,1000]\n",
    "labels = ['4.5','4.0','3.5','3.0','2.5','2.0','1.5','1.0','0.5']\n",
    "smp_dev_score['rating'] = pd.cut(smp_dev_score['score'], bins=bins, labels=labels, include_lowest=True)\n",
    "smp_full_score['rating'] = pd.cut(smp_full_score['score'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# DR by rating on dev sample\n",
    "DR_rating = smp_dev_score[['rating', target]].groupby(['rating']).agg(['count', 'sum' ])\n",
    "DR_rating = DR_rating.rename(columns={\"count\": \"Total\", \"sum\": \"Bads\"})\n",
    "DR_rating.columns = DR_rating.columns.droplevel(0)\n",
    "DR_rating['DR'] = (DR_rating['Bads'] / DR_rating['Total'])\n",
    "\n",
    "# DR by rating over time on full sample\n",
    "DR_rating_ot = smp_full_score[[date, 'rating', target]].groupby([date, 'rating']).agg(['count', 'sum' ])\n",
    "DR_rating_ot = DR_rating_ot.rename(columns={\"count\": \"Total\", \"sum\": \"Bads\"})\n",
    "DR_rating_ot.columns = DR_rating_ot.columns.droplevel(0)\n",
    "DR_rating_ot = DR_rating_ot.reset_index()\n",
    "DR_rating_ot['DR'] = (DR_rating_ot['Bads'] / DR_rating_ot['Total'])\n",
    "# set DR to null for last 12 months\n",
    "sorted_date = sorted(DR_rating_ot[date].unique())\n",
    "DR_rating_ot['DR'].loc[DR_rating_ot[date].isin(sorted_date[-12:])] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f3c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting results to excel\n",
    "writer = pd.ExcelWriter('7_1_testing_results.xlsx', engine='xlsxwriter')\n",
    "gini_ot.to_excel(writer, sheet_name='Gini_OT')\n",
    "gini_vars_train_test.to_excel(writer, sheet_name='Gini_Vars')\n",
    "gini_vars_ot.to_excel(writer, sheet_name='Gini_Vars_OT')\n",
    "train_distr.to_excel(writer, sheet_name='Distr_Train')\n",
    "test_distr.to_excel(writer, sheet_name='Distr_Test')\n",
    "psi_ot.to_excel(writer, sheet_name='PSI_OT')\n",
    "psi_vars_ot.to_excel(writer, sheet_name='PSI_Vars_OT')\n",
    "hhi_train_test.to_excel(writer, sheet_name='HHI')\n",
    "hhi_ot.to_excel(writer, sheet_name='HHI_OT')\n",
    "DR_rating.to_excel(writer, sheet_name='DR_rating')\n",
    "DR_rating_ot.to_excel(writer, sheet_name='DR_rating_ot')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfe2b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DR vs PD by product over time\n",
    "def pd_from_score(score, points0=540, odds0=1/9, pdo=40):\n",
    "    b = pdo/np.log(2)\n",
    "    a = points0 + b*np.log(odds0)\n",
    "    pd = 1/(1+np.exp(score/b - a/b))\n",
    "    return pd\n",
    "\n",
    "smp_dev_score['pd'] = pd_from_score(smp_dev_score['score'])\n",
    "\n",
    "dr_ot = smp_dev_score[[date, 'rating', target]].groupby([date, 'rating']).agg(['count', 'sum' ])\n",
    "dr_ot = dr_ot.rename(columns={\"count\": \"Total\", \"sum\": \"Bads\"})\n",
    "dr_ot.columns = dr_ot.columns.droplevel(0)\n",
    "dr_ot['Bad Rate'] = (dr_ot['Bads'] / dr_ot['Total'])\n",
    "\n",
    "dr_ot2 = smp_dev_score[[date, 'rating', 'pd']].groupby([date, 'rating']).agg(['count', 'sum' ])\n",
    "dr_ot2 = dr_ot2.rename(columns={\"count\": \"Total\", \"sum\": \"Bads\"})\n",
    "dr_ot2.columns = dr_ot2.columns.droplevel(0)\n",
    "dr_ot2['PD'] = (dr_ot2['Bads'] / dr_ot2['Total'])\n",
    "dr_ot['PD'] = dr_ot2['PD']\n",
    "\n",
    "dr_ot = dr_ot.swaplevel()\n",
    "\n",
    "dr_ot.to_excel('7_2_DR_ot_products.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4262cb7f",
   "metadata": {},
   "source": [
    "# 8. Recalibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2790c2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing sample for recalibration\n",
    "train_score  = sc.scorecard_ply(train, card, print_step=0)\n",
    "train_score[target] = train[target]\n",
    "train_score['pd_regr'] = sc.pd_from_score(train_score['score'])\n",
    "\n",
    "test_score  = sc.scorecard_ply(test, card, print_step=0)\n",
    "test_score[target] = test[target]\n",
    "test_score['pd_regr'] = sc.pd_from_score(test_score['score'])\n",
    "\n",
    "smp_calib_score = train_score.append(test_score)\n",
    "\n",
    "# assigning ratings\n",
    "bins = [0,500,540,580,620,660,700,740,780,1000]\n",
    "labels = ['4.5','4.0','3.5','3.0','2.5','2.0','1.5','1.0','0.5']\n",
    "smp_calib_score['rating'] = pd.cut(smp_calib_score['score'], bins=bins, labels=labels, include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d876055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibration(smp, score='score', target=target, points0=540, odds0=1/9, pdo=40):\n",
    "    b = pdo/np.log(2) \n",
    "    a = points0 + b*np.log(odds0)\n",
    "    log_odds = a/b - smp[score]/b\n",
    "    x = log_odds.to_numpy().reshape(-1, 1)\n",
    "    y = smp[target].to_numpy()\n",
    "    \n",
    "    lr_calib = LogisticRegression(penalty='none', solver='newton-cg', n_jobs=-1)\n",
    "    lr_calib.fit(x, y)\n",
    "\n",
    "    pd_calib = lr_calib.predict_proba(x_calib)[:,1]\n",
    "    log_odds_calib = np.log((1-pd_calib)/(pd_calib))\n",
    "\n",
    "    intercept_calib = points0 + (np.log(odds0) - lr_calib.intercept_) * pdo / np.log(2)\n",
    "    slope_calib = lr_calib.coef_ * pdo / np.log(2)\n",
    "    intercept_calib = intercept_calib[0]\n",
    "    slope_calib = slope_calib[0,0]\n",
    "\n",
    "    intercept = intercept_calib - slope_calib*a/b\n",
    "    slope = slope_calib/b\n",
    "    return intercept, slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1075ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept, slope = calibration(smp_calib_score, score='score', target=target)\n",
    "print(intercept, slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1feef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "smp_calib_score['score_new'] = smp_calib_score['score']*slope + intercept\n",
    "smp_calib_score['score_new'] = smp_calib_score['score_new'].astype(int)\n",
    "smp_calib_score['rating_new'] = pd.cut(smp_calib_score['score_new'], bins=bins, labels=labels, include_lowest=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
