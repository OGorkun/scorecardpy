{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff6aae60",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1f99dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946ebf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, precision_recall_curve, auc\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy.stats import chi2_contingency\n",
    "import scorecardpy as sc\n",
    "from scorecardpy.LogisticRegStats import LogisticRegStats\n",
    "import random as rd\n",
    "import re\n",
    "from IPython.display import display\n",
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fa21b6",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04f3004",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data prepare ------\n",
    "# load germancredit data\n",
    "smp_full = sc.germancredit()\n",
    "smp_full['will_default'] = smp_full['creditability'].apply(lambda x: 1 if x == 'bad' else 0)\n",
    "smp_full = smp_full.loc[:,smp_full.columns != 'creditability']\n",
    "smp_full.loc[0:99, 'credit.amount'] = np.nan\n",
    "smp_full.loc[0:99, 'purpose'] = np.nan\n",
    "smp_full.loc[100:109, 'will_default'] = np.nan\n",
    "\n",
    "for i in range(5):\n",
    "    smp_full = pd.concat([smp_full, smp_full])\n",
    "smp_full['RepDate_End'] = np.random.randint(1, 73, smp_full.shape[0])\n",
    "smp_full = smp_full.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ddf908",
   "metadata": {},
   "outputs": [],
   "source": [
    "smp_full = smp_full.rename(columns={\"will_default\": \"target\"})\n",
    "smp_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d718b37",
   "metadata": {},
   "source": [
    "# 1. Preliminary analysis of variables (missings, outliers, concentration/distribution) - based on smp_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630c601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns that are not variables\n",
    "var_skip = ['target','RepDate_End']\n",
    "# special values for numeric variables - TBD\n",
    "spl_val = []\n",
    "# list of variables by type (numeric variables with less than 10 unique values are considered as categorical)\n",
    "var_cat, var_num = sc.var_types(smp_full, var_skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676b89ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap for the missing values\n",
    "percent_missing = smp_full.loc[:, var_cat+var_num].isna().sum() * 100 / len(smp_full)\n",
    "percent_missing = pd.DataFrame({'column':percent_missing.index, 'percent_missing':percent_missing.values})\n",
    "percent_missing.sort_values('percent_missing', ascending=False, inplace=True)\n",
    "percent_missing.reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(smp_full[percent_missing.column].isna().transpose(),\n",
    "            cmap=\"YlGnBu\",\n",
    "            cbar_kws={'label': 'Missing Data'})\n",
    "plt.savefig(\"1_1_missings_heatmap.png\", dpi=100, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e025b9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#round missings\n",
    "#thresholds as params\n",
    "# warning checks\n",
    "var_cat_summary, var_num_summary = sc.var_pre_analysis(smp_full, var_cat, var_num, spl_val, hhi_low=0.05, hhi_high=0.95, min_share=0.05)\n",
    "\n",
    "writer = pd.ExcelWriter('1_2_preliminary_analysis.xlsx', engine='xlsxwriter')\n",
    "var_cat_summary.to_excel(writer, sheet_name='var_cat_summary')\n",
    "var_num_summary.to_excel(writer, sheet_name='var_num_summary')\n",
    "writer.save()\n",
    "\n",
    "display(var_cat_summary)\n",
    "display(var_num_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dba7c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#treatment of nan - median for numeric and 'Missing' for string\n",
    "for var, dt in smp_full.dtypes.items():\n",
    "    if var not in var_skip and smp_full[var].isna().sum() > 0:\n",
    "        print(var,smp_full[var].isna().sum()) \n",
    "        if dt.name == 'category':\n",
    "            smp_full[var] = smp_full[var].cat.add_categories('Missing').fillna('Missing')\n",
    "            print('Missing')\n",
    "        if dt.name == 'object':\n",
    "            smp_full[var] = smp_full[var].fillna('Missing')\n",
    "            print('Missing')\n",
    "        else: \n",
    "            print(smp_full[var].median())\n",
    "            smp_full[var] = smp_full[var].fillna(smp_full[var].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521906be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# distribution for categorical variables with extract to pdf\n",
    "sc.var_cat_distr(smp_full, var_cat, '1_3_categorical_vars_distribution.pdf', groupby='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93eac95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.var_num_distr(smp_full, var_num, '1_4_numerical_vars_distribution.pdf', groupby='target')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c723d2",
   "metadata": {},
   "source": [
    "# 2. Development sample creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0db2cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection of the development window \n",
    "sorted_date = sorted(smp_full['RepDate_End'].unique())\n",
    "del sorted_date[-12:]\n",
    "smp_dev = smp_full.loc[smp_full['RepDate_End'].isin(sorted_date)]\n",
    "# smp_dev = smp_full.loc[smp_full['RepDate_End'] < 20210700]\n",
    "# smp_dev = smp_full.loc[smp_full['RepDate_End'].isin(sorted_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee8303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check target\n",
    "print(smp_dev.groupby('target', dropna=False).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65ac1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete records with blank target\n",
    "smp_dev = smp_dev[smp_dev['target'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7011aca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection of variables that will be used for the development\n",
    "smp_dev = smp_dev[var_cat+var_num+['target']+['RepDate_End']]\n",
    "\n",
    "#smp_dev = smp_full.loc[smp_dev['prod_grp'] == 'Mortgage']\n",
    "\n",
    "# train/test split as 80/20\n",
    "train, test = sc.split_df(smp_dev, ratio=0.8, seed=123).values()\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e18fb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test sample size !update\n",
    "pd.DataFrame({'train':pd.Series(train.groupby('target', dropna=False).size()),\n",
    "              'test':pd.Series(test.groupby('target', dropna=False).size())})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7e95c3",
   "metadata": {},
   "source": [
    "# 3. Automated binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e152f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binning\n",
    "fine_class, coarse_class = sc.woebin(train, y = 'target', x = var_cat + var_num, init_count_distr = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec77cc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting binning results to excel !update\n",
    "pd.concat(fine_class.values()).reset_index(drop=True).to_excel('3_1_fine_classing.xlsx')\n",
    "pd.concat(coarse_class.values()).reset_index(drop=True).to_excel('3_2_coarse_classing_auto.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eb6448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iv for variables after automated binning !update\n",
    "coarse_class_iv = sc.vars_iv(var_cat + var_num, coarse_class)\n",
    "coarse_class_iv.to_excel('3_3_coarse_classing_auto_iv.xlsx')\n",
    "coarse_class_iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b41cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binning visualization\n",
    "# sc.woebin_plot(coarse_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af20d35d",
   "metadata": {},
   "source": [
    "# 4. Binning adjustments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e763453f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# manual review and adjustment of binning (first line should uncommented if manual adjustments are needed to be done) !update\n",
    "# breaks_list = sc.woebin_adj(train, y=\"target\", bins=coarse_class, fine_bins=fine_class, adj_all_var=False, save_breaks_list='3_4_breaks_list_adj.py')\n",
    "%run 3_4_breaks_list_adj.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29841728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables excluded based on binning results\n",
    "vars_bin_excl = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3984c9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update of coarse classing table (fine classing is relevant only for automated binning)\n",
    "fine_class_adj, coarse_class_adj = sc.woebin(train, y = 'target', x = var_cat + var_num, breaks_list = breaks_list, init_count_distr = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9f9fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying woe transformations on train and test samples \n",
    "train_woe = sc.woebin_ply(train, bins=coarse_class_adj)\n",
    "test_woe = sc.woebin_ply(test, bins=coarse_class_adj)\n",
    "\n",
    "# defining woe variables\n",
    "vars_woe = []\n",
    "for i in var_cat + var_num:\n",
    "    if i not in vars_bin_excl:\n",
    "        vars_woe.append(i+'_woe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3599e2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results of the final coarse classing after manual adjustments !update\n",
    "pd.concat(coarse_class_adj.values()).reset_index(drop=True).to_excel('3_5_coarse_classing_final.xlsx')\n",
    "coarse_class_adj_iv = sc.vars_iv(var_cat + var_num, coarse_class_adj)\n",
    "coarse_class_adj_iv.to_excel('3_6_coarse_classing_final_iv.xlsx')\n",
    "coarse_class_adj_iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bded02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IV for variables by defined subsamples (period, product etc.)\n",
    "sc.iv_group(train_woe, vars_woe, groupby='RepDate_End')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d796f435",
   "metadata": {},
   "source": [
    "# 5. Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48592c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "train_woe_corr = train_woe[vars_woe].corr()\n",
    "train_woe_corr.to_excel('5_1_correlation_matrix.xlsx')\n",
    "train_woe_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ba67d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting correlation heatmap\n",
    "plt.figure(figsize=(20,12))\n",
    "sns.heatmap(train_woe[vars_woe].corr(), cmap=\"YlGnBu\", annot=True)\n",
    "  \n",
    "# displaying heatmap\n",
    "plt.savefig('5_2_correlation_heatmap.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1febac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclusions by corr > 0.7\n",
    "vars_corr_excl = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce47aa0",
   "metadata": {},
   "source": [
    "# 6. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a4ae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# values in the test sample that are not present in train\n",
    "#print(np.am=ny(no.isnan(test_woe)))\n",
    "test_woe = test_woe.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7052a853",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_woe = []\n",
    "for i in var_cat + var_num:\n",
    "    if i not in vars_bin_excl + vars_corr_excl:\n",
    "        vars_woe.append(i+'_woe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9271812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target and variables\n",
    "y_train = train_woe['target']\n",
    "X_train = train_woe[vars_woe]\n",
    "y_test = test_woe['target']\n",
    "X_test = test_woe[vars_woe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07d2870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression ------\n",
    "lr = LogisticRegression(penalty='l1', C=0.9, solver='saga', n_jobs=-1)\n",
    "lr.fit(X_train, y_train)\n",
    "# lr.coef_\n",
    "# lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6949e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted proability\n",
    "train_pred = lr.predict_proba(X_train)[:,1]\n",
    "test_pred = lr.predict_proba(X_test)[:,1]\n",
    "# performance ks & roc ------\n",
    "train_perf = sc.perf_eva(y_train, train_pred, title = \"train\")\n",
    "test_perf = sc.perf_eva(y_test, test_pred, title = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7047c09a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train bad rate\n",
    "train_br = {}\n",
    "train_br['Total'] = y_train.count()\n",
    "train_br['Bads'] = int(y_train.sum())\n",
    "train_br['Bad Rate'] = round(train_br['Bads']/train_br['Total'], 4)\n",
    "# test bad rate\n",
    "test_br = {}\n",
    "test_br['Total'] = y_test.count()\n",
    "test_br['Bads'] = int(y_test.sum())\n",
    "test_br['Bad Rate'] = round(test_br['Bads']/test_br['Total'], 4)\n",
    "test_br\n",
    "# combining bad rate with performance\n",
    "perf = pd.DataFrame({'train':pd.Series(dict(list(train_br.items()) + list(train_perf.items()))),\n",
    "                         'test':pd.Series(dict(list(test_br.items()) + list(test_perf.items())))})\n",
    "perf = perf.loc[~perf.index.isin(['pic'])]\n",
    "perf.to_excel('6_1_perf_train_test.xlsx')\n",
    "perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976b635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression with stats\n",
    "lr2 = LogisticRegStats(penalty='l1', C=0.9, solver='saga', n_jobs=-1)\n",
    "lr2.fit(X_train, y_train)\n",
    "\n",
    "# calculating p-values and exporting to excel\n",
    "lr_output = pd.DataFrame({\n",
    "                'Variable': ['intercept'] + X_train.columns.tolist(),\n",
    "                'Coefficient': [lr2.model.intercept_[0]] + lr2.model.coef_[0].tolist(),\n",
    "                'P-value': [0] + lr2.p_values\n",
    "                })\n",
    "\n",
    "lr_output.to_excel('6_2_regr_output.xlsx')\n",
    "lr_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1c3d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score ------\n",
    "card = sc.scorecard(coarse_class_adj, lr, X_train.columns, start_zero=True)\n",
    "# credit score\n",
    "train_score  = sc.scorecard_ply(train, card, print_step=0)\n",
    "test_score = sc.scorecard_ply(test, card, print_step=0)\n",
    "# output to excel\n",
    "scorecard_points = pd.concat(card, ignore_index=True)\n",
    "scorecard_points.to_excel(\"6_3_scorecard_points.xlsx\", sheet_name='scorecard_points')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257f8e89",
   "metadata": {},
   "source": [
    "# 7. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e9ad5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# samples scoring\n",
    "smp_dev_score = sc.woebin_ply(smp_dev, bins=coarse_class_adj)\n",
    "smp_dev_score['score'] = sc.scorecard_ply(smp_dev, card, print_step=0)\n",
    "\n",
    "smp_full_score = sc.woebin_ply(smp_full, bins=coarse_class_adj)\n",
    "smp_full_score['score'] = sc.scorecard_ply(smp_full, card, print_step=0)\n",
    "\n",
    "# adding target\n",
    "train_score['target'] = train['target']\n",
    "test_score['target'] = test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406a53cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bad Rate over time\n",
    "gini_ot = smp_dev_score[['RepDate_End', 'target']].groupby(['RepDate_End']).agg(['count', 'sum' ])\n",
    "gini_ot = gini_ot.rename(columns={\"count\": \"Total\", \"sum\": \"Bads\"})\n",
    "gini_ot.columns = gini_ot.columns.droplevel(0)\n",
    "gini_ot['Bad Rate'] = (gini_ot['Bads'] / gini_ot['Total'])\n",
    "# adding Gini over time\n",
    "gini_ot['Gini'] = sc.gini_over_time(smp_dev_score, 'target', ['score'], 'RepDate_End')\n",
    "\n",
    "# Gini for vars train/test\n",
    "gini_vars_train = sc.gini_vars(train_woe, 'target', vars_woe, 'Train')\n",
    "gini_vars_test = sc.gini_vars(test_woe, 'target', vars_woe, 'Test')\n",
    "gini_vars_train_test = pd.merge(gini_vars_train, gini_vars_test, on = \"Variable\")\n",
    "\n",
    "# Gini for vars over time\n",
    "gini_vars_ot = sc.gini_over_time(smp_dev_score, 'target', vars_woe, 'RepDate_End')\n",
    "                                 \n",
    "# defining score ranges on train sample\n",
    "_, brk = pd.cut(train_score['score'], bins=10, retbins=True, duplicates='drop')\n",
    "brk = brk.round(decimals=2)\n",
    "brk = list(filter(lambda x: x>np.nanmin(train_score['score']) and x<np.nanmax(train_score['score']), brk))\n",
    "brk = [np.nanmin(smp_full_score['score'])] + sorted(brk) + [np.nanmax(smp_full_score['score'])]\n",
    "# applying score ranges on train, test adn full samples\n",
    "train_score['score_range'] = pd.cut(train_score['score'], bins=brk, include_lowest=False)\n",
    "test_score['score_range'] = pd.cut(test_score['score'], bins=brk, include_lowest=False)\n",
    "smp_full_score['score_range'] = pd.cut(smp_full_score['score'], bins=brk, include_lowest=False)\n",
    "# score distribution for train/test\n",
    "train_distr = sc.score_distr(train_score, 'target', 'score', 'score_range')\n",
    "test_distr = sc.score_distr(test_score, 'target', 'score', 'score_range')\n",
    "                                 \n",
    "# PSI over time (score_range) \n",
    "psi_ot = sc.psi_over_time(train_score, smp_full_score, ['score_range'], 'RepDate_End')\n",
    "                                 \n",
    "# PSI for WoE variables over time\n",
    "psi_vars_ot = sc.psi_over_time(train_woe, smp_full_score, vars_woe, 'RepDate_End')\n",
    "                                 \n",
    "# calculating hhi for train/test\n",
    "train_hhi = sc.hhi(train_score['score_range'].astype(str))\n",
    "test_hhi = sc.hhi(test_score['score_range'].astype(str))\n",
    "hhi_train_test = pd.DataFrame({\n",
    "        'train': [train_hhi], \n",
    "        'test': [test_hhi]\n",
    "    }, index = ['hhi'])\n",
    "                                 \n",
    "# calculating hhi over time\n",
    "hhi_ot = smp_full_score.groupby('RepDate_End').agg({'score_range': sc.hhi})\n",
    "hhi_ot = hhi_ot.rename(columns={\"score_range\": \"HHI\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ad6eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning ratings\n",
    "bins = [0,500,540,580,620,660,700,740,780,1000]\n",
    "labels = ['4.5','4.0','3.5','3.0','2.5','2.0','1.5','1.0','0.5']\n",
    "smp_dev_score['rating'] = pd.cut(smp_dev_score['score'], bins=bins, labels=labels, include_lowest=True)\n",
    "smp_full_score['rating'] = pd.cut(smp_full_score['score'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# DR by rating on dev sample\n",
    "DR_rating = smp_dev_score[['rating', 'target']].groupby(['rating']).agg(['count', 'sum' ])\n",
    "DR_rating = DR_rating.rename(columns={\"count\": \"Total\", \"sum\": \"Bads\"})\n",
    "DR_rating.columns = DR_rating.columns.droplevel(0)\n",
    "DR_rating['DR'] = (DR_rating['Bads'] / DR_rating['Total'])\n",
    "\n",
    "# DR by rating over time on full sample\n",
    "DR_rating_ot = smp_full_score[['RepDate_End', 'rating', 'target']].groupby(['RepDate_End', 'rating']).agg(['count', 'sum' ])\n",
    "DR_rating_ot = DR_rating_ot.rename(columns={\"count\": \"Total\", \"sum\": \"Bads\"})\n",
    "DR_rating_ot.columns = DR_rating_ot.columns.droplevel(0)\n",
    "DR_rating_ot = DR_rating_ot.reset_index()\n",
    "DR_rating_ot['DR'] = (DR_rating_ot['Bads'] / DR_rating_ot['Total'])\n",
    "# set DR to null for last 12 months\n",
    "sorted_date = sorted(DR_rating_ot['RepDate_End'].unique())\n",
    "DR_rating_ot['DR'].loc[DR_rating_ot['RepDate_End'].isin(sorted_date[-12:])] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f3c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting results to excel\n",
    "writer = pd.ExcelWriter('7_1_testing_results.xlsx', engine='xlsxwriter')\n",
    "gini_ot.to_excel(writer, sheet_name='Gini_OT')\n",
    "gini_vars_train_test.to_excel(writer, sheet_name='Gini_Vars')\n",
    "gini_vars_ot.to_excel(writer, sheet_name='Gini_Vars_OT')\n",
    "train_distr.to_excel(writer, sheet_name='Distr_Train')\n",
    "test_distr.to_excel(writer, sheet_name='Distr_Test')\n",
    "psi_ot.to_excel(writer, sheet_name='PSI_OT')\n",
    "psi_vars_ot.to_excel(writer, sheet_name='PSI_Vars_OT')\n",
    "hhi_train_test.to_excel(writer, sheet_name='HHI')\n",
    "hhi_ot.to_excel(writer, sheet_name='HHI_OT')\n",
    "DR_rating.to_excel(writer, sheet_name='DR_rating')\n",
    "DR_rating_ot.to_excel(writer, sheet_name='DR_rating_ot')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfe2b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DR vs PD by product over time\n",
    "def pd_from_score(score, points0=540, odds0=1/9, pdo=40):\n",
    "    b = pdo/np.log(2)\n",
    "    a = points0 + b*np.log(odds0)\n",
    "    pd = 1/(1+np.exp(score/b - a/b))\n",
    "    return pd\n",
    "\n",
    "smp_dev_score['pd'] = pd_from_score(smp_dev_score['score'])\n",
    "\n",
    "dr_ot = smp_dev_score[['RepDate_End', 'rating', 'target']].groupby(['RepDate_End', 'rating']).agg(['count', 'sum' ])\n",
    "dr_ot = dr_ot.rename(columns={\"count\": \"Total\", \"sum\": \"Bads\"})\n",
    "dr_ot.columns = dr_ot.columns.droplevel(0)\n",
    "dr_ot['Bad Rate'] = (dr_ot['Bads'] / dr_ot['Total'])\n",
    "\n",
    "dr_ot2 = smp_dev_score[['RepDate_End', 'rating', 'pd']].groupby(['RepDate_End', 'rating']).agg(['count', 'sum' ])\n",
    "dr_ot2 = dr_ot2.rename(columns={\"count\": \"Total\", \"sum\": \"Bads\"})\n",
    "dr_ot2.columns = dr_ot2.columns.droplevel(0)\n",
    "dr_ot2['PD'] = (dr_ot2['Bads'] / dr_ot2['Total'])\n",
    "dr_ot['PD'] = dr_ot2['PD']\n",
    "\n",
    "dr_ot = dr_ot.swaplevel()\n",
    "\n",
    "dr_ot.to_excel('7_2_DR_ot_products.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4262cb7f",
   "metadata": {},
   "source": [
    "# 8. Recalibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2790c2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing sample for recalibration\n",
    "train_score  = sc.scorecard_ply(train, card, print_step=0)\n",
    "train_score['target'] = train['target']\n",
    "train_score['pd_regr'] = sc.pd_from_score(train_score['score'])\n",
    "\n",
    "test_score  = sc.scorecard_ply(test, card, print_step=0)\n",
    "test_score['target'] = test['target']\n",
    "test_score['pd_regr'] = sc.pd_from_score(test_score['score'])\n",
    "\n",
    "smp_calib_score = train_score.append(test_score)\n",
    "\n",
    "# assigning ratings\n",
    "bins = [0,500,540,580,620,660,700,740,780,1000]\n",
    "labels = ['4.5','4.0','3.5','3.0','2.5','2.0','1.5','1.0','0.5']\n",
    "smp_calib_score['rating'] = pd.cut(smp_calib_score['score'], bins=bins, labels=labels, include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d876055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibration(smp, score='score', target='target', points0=540, odds0=1/9, pdo=40):\n",
    "    b = pdo/np.log(2) \n",
    "    a = points0 + b*np.log(odds0)\n",
    "    log_odds = a/b - smp[score]/b\n",
    "    x = log_odds.to_numpy().reshape(-1, 1)\n",
    "    y = smp[target].to_numpy()\n",
    "    \n",
    "    lr_calib = LogisticRegression(penalty='none', solver='newton-cg', n_jobs=-1)\n",
    "    lr_calib.fit(x, y)\n",
    "\n",
    "    pd_calib = lr_calib.predict_proba(x_calib)[:,1]\n",
    "    log_odds_calib = np.log((1-pd_calib)/(pd_calib))\n",
    "\n",
    "    intercept_calib = points0 + (np.log(odds0) - lr_calib.intercept_) * pdo / np.log(2)\n",
    "    slope_calib = lr_calib.coef_ * pdo / np.log(2)\n",
    "    intercept_calib = intercept_calib[0]\n",
    "    slope_calib = slope_calib[0,0]\n",
    "\n",
    "    intercept = intercept_calib - slope_calib*a/b\n",
    "    slope = slope_calib/b\n",
    "    return intercept, slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1075ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept, slope = calibration(smp_calib_score, score='score', target='target')\n",
    "print(intercept, slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1feef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "smp_calib_score['score_new'] = smp_calib_score['score']*slope + intercept\n",
    "smp_calib_score['score_new'] = smp_calib_score['score_new'].astype(int)\n",
    "smp_calib_score['rating_new'] = pd.cut(smp_calib_score['score_new'], bins=bins, labels=labels, include_lowest=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
